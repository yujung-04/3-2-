import streamlit as st
import pandas as pd
import altair as alt 
import numpy as np
import os
import json 
import re
import joblib 

# ë”¥ëŸ¬ë‹ ëª¨ë¸ í†µí•©ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
import warnings
warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')


# ğŸŒŸğŸŒŸğŸŒŸ Streamlit ì„¤ì •ì€ ëª¨ë“  st. ëª…ë ¹ ì¤‘ ê°€ì¥ ë¨¼ì € ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ğŸŒŸğŸŒŸğŸŒŸ
st.set_page_config(layout="wide")


# ====================================================================
# 0. ì „ì—­ ì„¤ì • ë° ëª¨ë¸ ê²½ë¡œ ì •ì˜
# ====================================================================

# ğŸš¨ 1. ë°ì´í„° ë¡œë“œ ê²½ë¡œ (í†µê³„ìš©)
DATA_PATH = '/Users/sunkyong/Downloads/petitions/data_categorized/train_categorized.csv' 

# ğŸš¨ 2. ìš”ì•½ ëª¨ë¸ ê²½ë¡œ (C íŒŒíŠ¸)
MODEL_LOCAL_PATH = "/Users/sunkyong/Downloads/petitions/final_models/kobart_summary_textrank_ft" 
device_sum = "cuda" if torch.cuda.is_available() else "cpu"

# ğŸš¨ 3. ë¶„ë¥˜ ëª¨ë¸ ì—ì…‹ ë””ë ‰í† ë¦¬ (B íŒŒíŠ¸)
CLASSIFY_MODEL_DIR = "/Users/sunkyong/Downloads/petitions/classify" 
EMBEDDER_MODEL_NAME = "jhgan/ko-sroberta-multitask"
device_clf = "cuda" if torch.cuda.is_available() else "cpu"

# ìµœì¢… ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
FULL_CATEGORY_MAP = {
    1: 'ê³¼í•™ê¸°ìˆ /ì •ë³´í†µì‹ ', 2: 'êµìœ¡', 3: 'êµ­í† /í•´ì–‘/êµí†µ', 4: 'ê¸°íƒ€',
    5: 'ë†ì—…/ì„ì—…/ìˆ˜ì‚°ì—…/ì¶•ì‚°ì—…', 6: 'ë¬¸í™”/ì²´ìœ¡/ê´€ê´‘/ì–¸ë¡ ', 7: 'ë³´ê±´ì˜ë£Œ', 8: 'ë³µì§€/ë³´í›ˆ',
    9: 'ì‚°ì—…/í†µìƒ', 10: 'ì†Œë¹„ì/ê³µì •ê±°ë˜', 11: 'ìˆ˜ì‚¬/ë²•ë¬´/ì‚¬ë²•ì œë„', 12: 'ì™¸êµ/í†µì¼/êµ­ë°©/ì•ˆë³´',
    13: 'ì¸ê¶Œ/ì„±í‰ë“±/ë…¸ë™', 14: 'ì¬ë‚œ/ì•ˆì „/í™˜ê²½', 15: 'ì¬ì •/ì„¸ì œ/ê¸ˆìœµ/ì˜ˆì‚°', 16: 'ì €ì¶œì‚°/ê³ ë ¹í™”/ì•„ë™/ì²­ì†Œë…„/ê°€ì¡±',
    17: 'ì •ì¹˜/ì„ ê±°/êµ­íšŒìš´ì˜', 18: 'í–‰ì •/ì§€ë°©ìì¹˜'
}

# ====================================================================
# A. ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ì •ì˜ (ìºì‹±)
# ====================================================================

@st.cache_data
def load_data(file_path):
    # í†µê³„ ë¶„ì„ ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)
    if 'agree_count' in df.columns:
        df['agree_count'] = df['agree_count'].astype(str).str.replace(',', '', regex=False)
        df['agree_count'] = pd.to_numeric(df['agree_count'], errors='coerce').fillna(0).astype(int)
    category_map = {k: v for k, v in FULL_CATEGORY_MAP.items()}
    df['category_name'] = df['category'].map(category_map)
    return df

@st.cache_resource
def load_classification_assets(model_dir):
    # B íŒŒíŠ¸ ë¶„ë¥˜ ëª¨ë¸ ì—ì…‹ ë¡œë“œ
    
    # ğŸš¨ğŸš¨ğŸš¨ ì„ì‹œ ì¬ì €ì¥ ë° ë¡œë“œ ë¡œì§ ì‹œì‘ ğŸš¨ğŸš¨ğŸš¨
    
    # 1. ì›ë³¸ íŒŒì¼ ë¡œë“œ ì‹œë„
    try:
        model = joblib.load(os.path.join(model_dir, 'classify_model.pkl'))
        scaler = joblib.load(os.path.join(model_dir, 'scaler.pkl'))
        
        # 2. ë¡œë“œ ì„±ê³µ ì‹œ í˜„ì¬ í™˜ê²½ì— ë§ê²Œ ìƒˆ íŒŒì¼ë¡œ ì¬ì €ì¥
        st.warning("ëª¨ë¸ ë¡œë“œ ì„±ê³µ! í˜„ì¬ í™˜ê²½ ë²„ì „ìœ¼ë¡œ '..._FIXED.pkl' íŒŒì¼ ì¬ì €ì¥ ì¤‘...")
        joblib.dump(model, os.path.join(model_dir, 'classify_model_FIXED.pkl'))
        joblib.dump(scaler, os.path.join(model_dir, 'scaler_FIXED.pkl'))
        st.success("âœ… ëª¨ë¸ íŒŒì¼ ì¬ì €ì¥ ì™„ë£Œ! ì´ì œ '..._FIXED.pkl'ì„ ì‚¬ìš©í•˜ë„ë¡ ì½”ë“œë¥¼ ë³€ê²½í•˜ì„¸ìš”.")
        
    except Exception as e:
        # 3. ë¡œë“œ ì‹¤íŒ¨ ì‹œ (BitGenerator ì˜¤ë¥˜ ë“±), ì´ë¯¸ ì¬ì €ì¥ëœ FIXED íŒŒì¼ì„ ë¡œë“œ ì‹œë„
        st.warning(f"âŒ ì›ë³¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}. ì¬ì €ì¥ëœ FIXED íŒŒì¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        
        try:
            model = joblib.load(os.path.join(model_dir, 'classify_model_FIXED.pkl'))
            scaler = joblib.load(os.path.join(model_dir, 'scaler_FIXED.pkl'))
            st.info("âœ… FIXED íŒŒì¼ ë¡œë“œ ì„±ê³µ.")
        except FileNotFoundError:
             st.error("âŒ classify_model_FIXED.pkl ë˜ëŠ” scaler_FIXED.pkl íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
             return None, None, None, None
        except Exception as e_fixed:
             st.error(f"âŒ FIXED íŒŒì¼ ë¡œë“œë„ ì‹¤íŒ¨: {e_fixed}")
             return None, None, None, None
             
    # ğŸš¨ğŸš¨ğŸš¨ ì„ì‹œ ì¬ì €ì¥ ë° ë¡œë“œ ë¡œì§ ë ğŸš¨ğŸš¨ğŸš¨
    
    # ë‚˜ë¨¸ì§€ ì—ì…‹ ë¡œë“œ (ì˜¤ë¥˜ ì—†ì´ ì§„í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.)
    try:
        embedder = SentenceTransformer(EMBEDDER_MODEL_NAME) # SBERT ì„ë² ë”© ëª¨ë¸
        with open(os.path.join(model_dir, 'label_list.json'), 'r') as f:
            label_list = json.load(f)
        return model, scaler, embedder, label_list
    except Exception as e:
        st.error(f"âŒ ì„ë² ë”/ë¼ë²¨ ëª©ë¡ ë¡œë“œ ì‹¤íŒ¨ (SBERT ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¬¸ì œì¼ ìˆ˜ ìˆìŒ): {e}")
        return None, None, None, None

@st.cache_resource
def load_summarization_model(model_path):
    # C íŒŒíŠ¸ ìš”ì•½ ëª¨ë¸ ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device_sum)
    model.eval()
    return tokenizer, model



# ====================================================================
# B. ëª¨ë¸ ë¡œë“œ ì‹¤í–‰ ë° ì¶”ë¡  í•¨ìˆ˜ ì •ì˜
# ====================================================================

# ë¡œë“œ ì‹¤í–‰
data_df = load_data(DATA_PATH)
model_clf, scaler_clf, embedder_clf, label_list_clf = load_classification_assets(CLASSIFY_MODEL_DIR)

# ìš”ì•½ ëª¨ë¸ì€ ì„ì‹œ ë¹„í™œì„± ìƒíƒœ ìœ ì§€
try:
    tokenizer_sum, model_sum = load_summarization_model(MODEL_LOCAL_PATH)
    SUMMARY_LOADED = True
except Exception:
    SUMMARY_LOADED = False

# 1. ë¶„ë¥˜ ì¶”ë¡  í•¨ìˆ˜ (B íŒŒíŠ¸ ë¡œì§)
def classify_petition(text):
    if not model_clf or not embedder_clf:
        return -1, "ë¡œë“œ ì‹¤íŒ¨ (ì—ì…‹ í™•ì¸ í•„ìš”)"
        
    processed_text = text.strip() 
    
    # 2. SBERT ì„ë² ë”© (í…ìŠ¤íŠ¸ -> ë²¡í„° ë³€í™˜)
    text_vector = embedder_clf.encode([processed_text], convert_to_numpy=True)
    
    # 3. Scaler ì ìš© (ë°ì´í„° ì •ê·œí™”)
    text_scaled = scaler_clf.transform(text_vector)
    
    # 4. ì˜ˆì¸¡
    prediction_index = model_clf.predict(text_scaled)[0]
    
    # 5. ì˜ˆì¸¡ëœ ì¸ë±ìŠ¤ -> ì¹´í…Œê³ ë¦¬ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
    try:
        category_code = label_list_clf[prediction_index] 
        category_name = FULL_CATEGORY_MAP.get(category_code, "ë¶„ë¥˜ ë¶ˆê°€ëŠ¥")
        return category_code, category_name
    except IndexError:
        return -1, "ë§¤í•‘ ì˜¤ë¥˜ (IndexError)"


# 2. ìš”ì•½ ì¶”ë¡  í•¨ìˆ˜ (C íŒŒíŠ¸ ë¡œì§)
def summarize_petition(text, max_len=150):
    if not SUMMARY_LOADED:
        return "C íŒŒíŠ¸ì˜ ìš”ì•½ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ë¡œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    inputs = tokenizer_sum(
        text,
        return_tensors="pt",
        max_length=512,
        truncation=True
    ).to(device_sum)


    with torch.no_grad():
        summary_ids = model_sum.generate(
            **inputs,
            max_length=max_len,
            min_length=40,
            num_beams=4,
            no_repeat_ngram_size=3,
            repetition_penalty=2.0,
            length_penalty=1.0,
            early_stopping=True,
        )
    return tokenizer_sum.decode(summary_ids[0], skip_special_tokens=True)


# ====================================================================
# C. Streamlit ëŒ€ì‹œë³´ë“œ í™”ë©´ êµ¬ì„±
# ====================================================================

st.title("ğŸ‡°ğŸ‡· êµ­íšŒ ì²­ì› ë°ì´í„° ë¶„ì„ ë° AI ëª¨ë¸ ëŒ€ì‹œë³´ë“œ")
st.markdown("### ğŸ“ AíŒŒíŠ¸: ìµœì¢… ì‹œìŠ¤í…œ í†µí•©")

tab1, tab2, tab3 = st.tabs(["ë°ì´í„° í†µê³„ ë° ì‹œê°í™”", "AI ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ë¶„ë¥˜)", "AI ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ìš”ì•½)"])

with tab1:
    st.header("1. ë°ì´í„° í†µê³„ ë° ì£¼ìš” í˜„í™©")
    
    st.subheader("1-1. ì¹´í…Œê³ ë¦¬ë³„ ì²­ì› ìˆ˜")
    category_counts = data_df['category_name'].value_counts().reset_index()
    category_counts.columns = ['Category', 'Count']
    chart1 = alt.Chart(category_counts).mark_bar().encode(
        x=alt.X('Count', title='ì²­ì› ìˆ˜'),
        y=alt.Y('Category', sort='-x', title='ì²­ì› ì¹´í…Œê³ ë¦¬'),
        tooltip=['Category', 'Count']
    ).properties(title='ì¹´í…Œê³ ë¦¬ë³„ ì²­ì› ê±´ìˆ˜ ë¶„í¬').interactive()
    st.altair_chart(chart1, use_container_width=True)


    st.subheader("1-2. ë™ì˜ ì¸ì›ìˆ˜ Top 10 ì²­ì›")
    top_agree = data_df.nlargest(10, 'agree_count')
    st.dataframe(top_agree[['title', 'category_name', 'agree_count']], use_container_width=True)

    
    st.subheader("1-3. ë™ì˜ ì¸ì›ìˆ˜ ë¶„í¬ (íˆìŠ¤í† ê·¸ë¨)")
    st.caption("ëŒ€ë¶€ë¶„ì˜ ì²­ì›ì´ ë‚®ì€ ë™ì˜ìˆ˜ë¥¼ ê°€ì§€ë¯€ë¡œ ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜í•˜ì—¬ ë¶„í¬ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.")
    log_agree_count = data_df['agree_count'].apply(lambda x: np.log10(x + 1))
    chart2 = alt.Chart(pd.DataFrame({'log_agree_count': log_agree_count})).mark_bar().encode(
        alt.X("log_agree_count", bin=alt.Bin(maxbins=30), title="Log10(ë™ì˜ ì¸ì› ìˆ˜ + 1)"),
        alt.Y("count()", title="ì²­ì› ìˆ˜"),
        tooltip=[alt.Tooltip("log_agree_count", bin=True), "count()"]
    ).properties(title='ë™ì˜ ì¸ì›ìˆ˜ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨').interactive()
    st.altair_chart(chart2, use_container_width=True)


with tab2:
    st.header("2. AI ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ë¶„ë¥˜)")
    st.markdown("---")
    
    st.subheader("2-1. ë¶„ë¥˜ ëª¨ë¸ ì¶”ë¡  ìë¦¬")
    input_text = st.text_area("ì²­ì› ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”. (ì œëª© ì—†ì´ ë‚´ìš©ë§Œ)", height=300, key="input_text_classify")
    classify_button = st.button("ë¶„ë¥˜ ì‹¤í–‰", key="classify_button", type="primary")
    
    st.markdown("#### ì˜ˆì¸¡ ê²°ê³¼:")
    
    if classify_button:
        if input_text:
            if model_clf and embedder_clf:
                with st.spinner('ë¶„ë¥˜ ì¤‘ì…ë‹ˆë‹¤...'):
                    pred_code, pred_name = classify_petition(input_text)
                    
                st.success("âœ… ì˜ˆì¸¡ ì™„ë£Œ")
                st.info(f"ì˜ˆì¸¡ ì¹´í…Œê³ ë¦¬ ì½”ë“œ: **{pred_code}**")
                st.markdown(f"**ìµœì¢… ë¶„ë¥˜ ê²°ê³¼:** ## {pred_name}")
            else:
                st.warning("ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì—ì…‹ íŒŒì¼ ë° ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            st.warning("ë¶„ë¥˜í•  ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")


with tab3:
    st.header("3. AI ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ìš”ì•½)")
    st.markdown("---")
    
    st.subheader("3-1. ìš”ì•½ ëª¨ë¸ ì¶”ë¡  ìë¦¬")
    petition_text = st.text_area(
        "ìš”ì•½í•  ì²­ì› ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”.", 
        height=300, 
        key="input_text_summarize"
    )
    summarize_button = st.button("ìš”ì•½ ì‹¤í–‰", key="summarize_button", type="primary")
    
    st.markdown("#### ìš”ì•½ ê²°ê³¼:")
    
    if summarize_button:
        if petition_text:
            if SUMMARY_LOADED:
                with st.spinner('ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...'):
                    final_summary = summarize_petition(petition_text)
                    
                st.success("âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ")
                st.info(final_summary)
            else:
                st.warning("ìš”ì•½ ëª¨ë¸ ë¡œë“œì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            st.warning("ìš”ì•½í•  ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")